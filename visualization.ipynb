{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c46f9d1-ab23-4e3c-9d4c-a085b7324c58",
   "metadata": {},
   "source": [
    "# Yorùbá Sentiment Analysis Results Visualization\n",
    "\n",
    "This notebook visualizes the results of our Yorùbá sentiment analysis model based on AfriBERTa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4f016-48f7-486c-a7ae-4588d90eb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Set larger figure sizes and font sizes for better readability\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91ff6a-741c-4d76-85ed-615d7a68ea92",
   "metadata": {},
   "source": [
    "## Loading Model Results\n",
    "\n",
    "First, we'll load the model results from the log files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569da11d-73ed-4309-8b6f-a02f67ced25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model logs\n",
    "def load_model_logs(logs_dir=\"logs\"):\n",
    "    model_logs = []\n",
    "    \n",
    "    for filename in os.listdir(logs_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(logs_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    model_logs.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    return model_logs\n",
    "\n",
    "# Load the model logs\n",
    "model_logs = load_model_logs()\n",
    "\n",
    "print(f\"Loaded {len(model_logs)} model logs\")\n",
    "\n",
    "# Create a DataFrame with the main metrics\n",
    "metrics_data = []\n",
    "\n",
    "for log in model_logs:\n",
    "    model_name = f\"{log['model_name']}_{log['timestamp']}\"\n",
    "    metrics = log.get(\"metrics\", {})\n",
    "    \n",
    "    data_row = {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": metrics.get(\"accuracy\", 0) * 100,  # Convert to percentage\n",
    "        \"precision\": metrics.get(\"precision\", 0) * 100,\n",
    "        \"recall\": metrics.get(\"recall\", 0) * 100,\n",
    "        \"f1\": metrics.get(\"f1\", 0) * 100,\n",
    "        \"sample_test_accuracy\": metrics.get(\"sample_test_accuracy\", 0) * 100\n",
    "    }\n",
    "    \n",
    "    # Parse confusion matrix if available in results\n",
    "    if \"sample_test_results\" in log:\n",
    "        true_labels = [result[\"true_label\"] for result in log[\"sample_test_results\"]]\n",
    "        pred_labels = [result[\"predicted_label\"] for result in log[\"sample_test_results\"]]\n",
    "        data_row[\"true_labels\"] = true_labels\n",
    "        data_row[\"pred_labels\"] = pred_labels\n",
    "        \n",
    "    metrics_data.append(data_row)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3278a-dac7-4f4b-9206-21c43d0a98cc",
   "metadata": {},
   "source": [
    "## 1. Correlation Heatmap of Metrics\n",
    "\n",
    "This shows how different metrics correlate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b2f96-bffa-4597-9577-de1bc43691fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation\n",
    "numeric_columns = ['accuracy', 'precision', 'recall', 'f1', 'sample_test_accuracy']\n",
    "correlation_matrix = metrics_df[numeric_columns].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    vmin=-1, vmax=1,\n",
    "    fmt='.2f',\n",
    "    linewidths=1,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": .8},\n",
    ")\n",
    "plt.title('Correlation Heatmap of Model Metrics', fontsize=18, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d24d5-e2de-4099-88b4-cf587391bbeb",
   "metadata": {},
   "source": [
    "## 2. Model Comparison - Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92b5c7-711d-4729-a9f3-256c96285113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using parts of the original name, assign simple sequential labels\n",
    "metrics_df['model_name'] = [f\"Model {i+1}\" for i in range(len(metrics_df))]\n",
    "\n",
    "# Create bar chart for accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=metrics_df,\n",
    "    x='model_name',\n",
    "    y='accuracy',\n",
    "    palette='Blues_d',\n",
    "    width=0.5,\n",
    "    hue='model_name',\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(metrics_df['accuracy']):\n",
    "    plt.text(\n",
    "        i, v + 1, \n",
    "        f\"{v:.2f}%\", \n",
    "        ha='center', \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.title('Model Accuracy Comparison', fontsize=18, pad=20)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100%\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=25)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a1be2-288c-40cf-8a3f-1e2e9fb1a6ce",
   "metadata": {},
   "source": [
    "## 3. Model Comparison - Precision Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800b233-0c57-4686-aa58-f258e98c875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for precision\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=metrics_df,\n",
    "    x='model_name',\n",
    "    y='precision',\n",
    "    width=0.5,\n",
    "    palette='Greens_d',\n",
    "    hue='model_name',\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(metrics_df['precision']):\n",
    "    plt.text(\n",
    "        i, v + 1, \n",
    "        f\"{v:.2f}%\", \n",
    "        ha='center', \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.title('Model Precision Comparison', fontsize=18, pad=20)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Precision (%)')\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100%\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_precision_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bd129-5c0c-486c-afbd-bb3af0d06fef",
   "metadata": {},
   "source": [
    "## 4. Model Comparison - Recall Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4fe70-f920-440a-83be-30fe1dc523a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for recall\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=metrics_df,\n",
    "    x='model_name',\n",
    "    y='recall',\n",
    "    palette='Oranges_d',\n",
    "    width=0.5,\n",
    "    hue='model_name',\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(metrics_df['recall']):\n",
    "    plt.text(\n",
    "        i, v + 1, \n",
    "        f\"{v:.2f}%\", \n",
    "        ha='center', \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.title('Model Recall Comparison', fontsize=18, pad=20)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall (%)')\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100%\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_recall_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf893f5",
   "metadata": {},
   "source": [
    "## 5. Model Comparison - F1 Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a new bar chart for F1 scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=metrics_df,\n",
    "    x='model_name',\n",
    "    y='f1',\n",
    "    palette='Purples_d',  # Using a different color palette\n",
    "    width=0.5,\n",
    "    hue='model_name',\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(metrics_df['f1']):\n",
    "    plt.text(\n",
    "        i, v + 1, \n",
    "        f\"{v:.2f}%\", \n",
    "        ha='center', \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.title('Model F1 Score Comparison', fontsize=18, pad=20)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score (%)')\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100%\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_f1_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056b5ae-aee1-4090-b568-a5c477864e31",
   "metadata": {},
   "source": [
    "## 6. Model Comparison - Accuracy, Precision and Recall Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d45f21-3adb-49a8-8b64-fcb8c7b2e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for plotting multiple metrics together\n",
    "metrics_melted = pd.melt(\n",
    "    metrics_df, \n",
    "    id_vars=['model', 'model_name'], \n",
    "    value_vars=['accuracy', 'precision', 'recall', 'f1'],\n",
    "    var_name='metric', \n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Create a line graph with a very narrow y-axis range\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Use distinct marker styles, colors, and line styles for each metric\n",
    "markers = ['o', 's', 'D', '^']  # circle, square, diamond, triangle\n",
    "colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#9467bd']  # blue, green, orange, purple\n",
    "linestyles = ['-', '-', '-', '-']  # solid, dashed, dash-dot, dotted\n",
    "zorders = [4, 3, 2, 1]  # Higher zorder will be on top\n",
    "\n",
    "# Plot each metric as a line with variations to prevent visual overlapping\n",
    "for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
    "    metric_data = metrics_melted[metrics_melted['metric'] == metric]\n",
    "    \n",
    "    # Apply a tiny offset to separate overlapping lines\n",
    "    # This is purely visual and doesn't change the data interpretation\n",
    "    offset = i * 0.05  # Small offset for visual separation\n",
    "    \n",
    "    plt.plot(\n",
    "        metric_data['model_name'], \n",
    "        metric_data['score'] + offset,  # Add small offset for visualization\n",
    "        marker=markers[i],\n",
    "        linestyle=linestyles[i],\n",
    "        linewidth=3 + (3-i)*0.5,  # Vary line thickness\n",
    "        markersize=12 + (3-i)*1,  # Vary marker size\n",
    "        color=colors[i],\n",
    "        label=f\"{metric.capitalize()} (+{offset:.2f}% visual offset)\",  # Note offset in legend\n",
    "        zorder=zorders[i]  # Control which lines appear on top\n",
    "    )\n",
    "    \n",
    "    # Add value labels with the original (non-offset) values\n",
    "    for x, y in zip(metric_data['model_name'], metric_data['score']):\n",
    "        plt.text(\n",
    "            x, y + offset + 0.15,  # Offset text position but show original value\n",
    "            f\"{y:.2f}%\", \n",
    "            ha='center',\n",
    "            fontsize=10,\n",
    "            fontweight='bold',\n",
    "            color=colors[i]\n",
    "        )\n",
    "\n",
    "# Set an extremely narrow y-axis range to highlight minimal differences\n",
    "# Add room for the offsets\n",
    "plt.ylim(68.5, 73.0)  # Set to just below min and just above max + offsets\n",
    "\n",
    "plt.title('Model Metrics Comparison (Zoomed In)', fontsize=18, pad=20)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score (%)', fontsize=14)\n",
    "\n",
    "# Add very fine horizontal grid lines\n",
    "plt.grid(axis='y', which='major', alpha=0.3, linestyle='-')\n",
    "plt.grid(axis='y', which='minor', alpha=0.15, linestyle='--')\n",
    "plt.minorticks_on()\n",
    "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))  # Add grid lines every 0.1%\n",
    "\n",
    "# Add a legend with better positioning and explanation\n",
    "plt.legend(\n",
    "    title='Metrics (with visual offsets)',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.28),\n",
    "    ncol=2,  # Use two rows instead of one for better readability\n",
    "    fontsize=10,\n",
    "    title_fontsize=12,\n",
    "    framealpha=0.9,  # More opaque background\n",
    ")\n",
    "\n",
    "# Add annotations explaining the visualization choices\n",
    "plt.figtext(\n",
    "    0.5, 0.01, \n",
    "    \"Note: Y-axis is zoomed in (range: 69-72%). Small offsets added to each metric line for better visibility. Original values shown in labels.\",\n",
    "    ha='center',\n",
    "    fontsize=10,\n",
    "    fontstyle='italic',\n",
    "    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7)\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])  # Make room for the note at bottom\n",
    "plt.savefig('model_metrics_comparison_zoomed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a155623-feac-4ea8-8bf4-636cdfcf4d83",
   "metadata": {},
   "source": [
    "## 7. ROC Curves for Sentiment Analysis\n",
    "\n",
    "This visualization shows ROC curves for each model. Note that for multiclass classification (positive, negative, neutral), we'll need to create a one-vs-rest ROC curve for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7eb6e-608f-4e89-8e9b-b367ea712088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to extract predictions and true labels\n",
    "def extract_multiclass_predictions(log_entry):\n",
    "    \"\"\"Extract predictions with confidence scores for ROC analysis\"\"\"\n",
    "    results = log_entry.get(\"sample_test_results\", [])\n",
    "    if not results:\n",
    "        return None, None\n",
    "    \n",
    "    # Get class labels\n",
    "    classes = sorted(list(set([r[\"true_label\"] for r in results])))\n",
    "    class_to_idx = {label: idx for idx, label in enumerate(classes)}\n",
    "    \n",
    "    # Extract true labels and predictions\n",
    "    y_true = [class_to_idx[r[\"true_label\"]] for r in results]\n",
    "    \n",
    "    # For each class, get confidence scores\n",
    "    y_scores = []\n",
    "    for r in results:\n",
    "        # If probabilities for each class aren't available, we'll use the confidence\n",
    "        # score only for the predicted class\n",
    "        confidence = r.get(\"confidence\", 0.5)\n",
    "        pred_label = r[\"predicted_label\"]\n",
    "        pred_idx = class_to_idx[pred_label]\n",
    "        \n",
    "        # Create a score vector with confidence for predicted class\n",
    "        scores = [0.0] * len(classes)\n",
    "        scores[pred_idx] = confidence\n",
    "        y_scores.append(scores)\n",
    "    \n",
    "    return np.array(y_true), np.array(y_scores), classes\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Process each model with sample test results\n",
    "for log in model_logs:\n",
    "    model_name = f\"Model {model_logs.index(log) + 1}\"\n",
    "        \n",
    "    # Extract predictions and true labels\n",
    "    y_true, y_scores, classes = extract_multiclass_predictions(log)\n",
    "        \n",
    "    if y_true is None or len(y_true) == 0:\n",
    "        print(f\"No valid prediction data found for {model_name}\")\n",
    "        continue\n",
    "        \n",
    "    # Binarize the labels for multi-class ROC\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(classes)))\n",
    "        \n",
    "    # Plot ROC curve for each class\n",
    "    for i, class_name in enumerate(classes):\n",
    "        # Get scores for this class\n",
    "        class_scores = y_scores[:, i]\n",
    "            \n",
    "        # Compute ROC curve and area\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], class_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "        # Plot the curve\n",
    "        plt.plot(\n",
    "            fpr, tpr, \n",
    "            lw=2, \n",
    "            label=f'{model_name}... - {class_name} (AUC = {roc_auc:.2f})'\n",
    "        )\n",
    "\n",
    "# Plot the diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Sentiment Analysis Models', fontsize=18, pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# If we have issues with ROC curve generation, we can use a simpler approach with confusion matrices instead\n",
    "def plot_confusion_matrices(model_logs):\n",
    "    \"\"\"Plot confusion matrices for each model\"\"\"\n",
    "    for log in model_logs:\n",
    "        model_name = f\"Model {model_logs.index(log) + 1}\"\n",
    "        results = log.get(\"sample_test_results\", [])\n",
    "            \n",
    "        if not results:\n",
    "            continue\n",
    "                \n",
    "        true_labels = [r[\"true_label\"] for r in results]\n",
    "        pred_labels = [r[\"predicted_label\"] for r in results]\n",
    "            \n",
    "        # Get unique classes\n",
    "        classes = sorted(list(set(true_labels + pred_labels)))\n",
    "            \n",
    "        # Convert string labels to indices\n",
    "        class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "        y_true = [class_to_idx[label] for label in true_labels]\n",
    "        y_pred = [class_to_idx[label] for label in pred_labels]\n",
    "            \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "            \n",
    "        # Normalize the confusion matrix\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            cm_norm, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='Blues',\n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes\n",
    "        )\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_{model_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Plot confusion matrices as backup\n",
    "plot_confusion_matrices(model_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab5519-3e88-4d2d-bc42-dcf1cff1809a",
   "metadata": {},
   "source": [
    "## 8. Training Loss Curves\n",
    "\n",
    "Let's visualize the training loss over epochs to see how training progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8680b-1d45-4213-ab75-a1d676f7c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined visualization for training loss across all models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create a separate figure for validation accuracy\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Track figures to plot data on the correct one\n",
    "loss_fig = plt.figure(1)\n",
    "accuracy_fig = plt.figure(2)\n",
    "\n",
    "# Use different colors and line styles for each model\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "linestyles = ['-', '--', '-.', ':', '-', '--']\n",
    "\n",
    "# Track whether any models had validation metrics\n",
    "has_validation_metrics = False\n",
    "\n",
    "# Iterate through models and add to combined plots\n",
    "for idx, log in enumerate(model_logs):\n",
    "    color = colors[idx % len(colors)]  # Cycle through colors if more models than colors\n",
    "    linestyle = linestyles[idx % len(linestyles)]\n",
    "    \n",
    "    model_name = f\"Model {idx+1}\"  # Simplified model name for legend\n",
    "    training_info = log.get('training_info', {})\n",
    "    epochs_data = training_info.get('epochs', [])\n",
    "    \n",
    "    if not epochs_data:\n",
    "        print(f\"No training data found for {model_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract epoch numbers and training loss\n",
    "    epochs = [epoch_data.get('epoch', i+1) for i, epoch_data in enumerate(epochs_data)]\n",
    "    train_losses = [epoch_data.get('train_loss', 0) for epoch_data in epochs_data]\n",
    "    \n",
    "    # Add to the training loss plot\n",
    "    plt.figure(1)  # Select the loss figure\n",
    "    plt.plot(\n",
    "        epochs, train_losses, \n",
    "        marker='o', \n",
    "        linestyle=linestyle,\n",
    "        linewidth=2, \n",
    "        markersize=8,\n",
    "        color=color,\n",
    "        label=model_name\n",
    "    )\n",
    "    \n",
    "    # Check if validation metrics are available\n",
    "    if 'accuracy' in epochs_data[0]:\n",
    "        has_validation_metrics = True\n",
    "        val_accuracy = [epoch_data.get('accuracy', 0) * 100 for epoch_data in epochs_data]\n",
    "        \n",
    "        # Add to the validation accuracy plot\n",
    "        plt.figure(2)  # Select the accuracy figure\n",
    "        plt.plot(\n",
    "            epochs, val_accuracy, \n",
    "            marker='s',  # Square markers to distinguish from loss curves\n",
    "            linestyle=linestyle,\n",
    "            linewidth=2, \n",
    "            markersize=8,\n",
    "            color=color,\n",
    "            label=model_name\n",
    "        )\n",
    "\n",
    "# Finalize the training loss plot\n",
    "plt.figure(1)\n",
    "plt.title('Training Loss Comparison Across Models', fontsize=18, pad=20)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Training Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('combined_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Finalize the validation accuracy plot if we have validation metrics\n",
    "if has_validation_metrics:\n",
    "    plt.figure(2)\n",
    "    plt.title('Validation Accuracy Comparison Across Models', fontsize=18, pad=20)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Validation Accuracy (%)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('combined_validation_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No validation metrics available for any model\")\n",
    "\n",
    "# Also keep the individual plots for detailed per-model analysis\n",
    "for idx, log in enumerate(model_logs):\n",
    "    model_name = f\"{log['model_name']}_{log['timestamp']}\"\n",
    "    training_info = log.get('training_info', {})\n",
    "    epochs_data = training_info.get('epochs', [])\n",
    "    \n",
    "    if not epochs_data:\n",
    "        continue\n",
    "    \n",
    "    # Extract epoch numbers and training loss\n",
    "    epochs = [epoch_data.get('epoch', i+1) for i, epoch_data in enumerate(epochs_data)]\n",
    "    train_losses = [epoch_data.get('train_loss', 0) for epoch_data in epochs_data]\n",
    "    \n",
    "    # Plot individual training loss curve\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(epochs, train_losses, 'o-', linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.title(f'Training Loss - {model_name}', fontsize=18, pad=20)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_loss_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # If we have validation metrics, plot those too individually\n",
    "    if 'accuracy' in epochs_data[0]:\n",
    "        val_accuracy = [epoch_data.get('accuracy', 0) * 100 for epoch_data in epochs_data]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(epochs, val_accuracy, 'o-', linewidth=2, markersize=8, color='green')\n",
    "        \n",
    "        plt.title(f'Validation Accuracy - {model_name}', fontsize=18, pad=20)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation Accuracy (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'validation_accuracy_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a184860-e26f-4292-a0f7-904850ffdd9b",
   "metadata": {},
   "source": [
    "## 9. Class Distribution Analysis\n",
    "\n",
    "Let's analyze the distribution of sentiment classes in our test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794b345-a485-4f46-8c20-cbdd1517389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution in test results\n",
    "for log in model_logs:\n",
    "    model_name = f\"{log['model_name']}_{log['timestamp']}\"\n",
    "    results = log.get(\"sample_test_results\", [])\n",
    "    \n",
    "    if not results:\n",
    "        continue\n",
    "        \n",
    "    # Count occurrences of each class\n",
    "    true_labels = [r[\"true_label\"] for r in results]\n",
    "    pred_labels = [r[\"predicted_label\"] for r in results]\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_label': true_labels,\n",
    "        'predicted_label': pred_labels,\n",
    "        'correct': [r.get(\"correct\", False) for r in results],\n",
    "        'confidence': [r.get(\"confidence\", 0) for r in results]\n",
    "    })\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # True label distribution\n",
    "    sns.countplot(data=results_df, x='true_label', ax=ax1)\n",
    "    ax1.set_title('True Label Distribution')\n",
    "    ax1.set_xlabel('Sentiment Class')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Predicted label distribution\n",
    "    sns.countplot(data=results_df, x='predicted_label', ax=ax2)\n",
    "    ax2.set_title('Predicted Label Distribution')\n",
    "    ax2.set_xlabel('Sentiment Class')\n",
    "    ax2.set_ylabel('Count')\n",
    "    \n",
    "    plt.suptitle(f'Class Distribution - {model_name}', fontsize=18, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'class_distribution_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Confidence analysis by class\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=results_df, x='predicted_label', y='confidence')\n",
    "    plt.title(f'Prediction Confidence by Class - {model_name}', fontsize=18, pad=20)\n",
    "    plt.xlabel('Predicted Sentiment Class')\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'confidence_by_class_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
